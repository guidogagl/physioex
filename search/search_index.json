{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>  PhysioEx  </p> <p> </p> <p>PhysioEx ( Physiological Signal Explainer ) is a versatile python library tailored for building, training, and explaining deep learning models for physiological signal analysis. </p> <p>The main purpose of the library is to propose a standard and fast methodology to train and evalutate state-of-the-art deep learning architectures for physiological signal analysis, to shift the attention from the architecture building task to the explainability task. </p> <p>With PhysioEx you can simulate a state-of-the-art experiment just running the <code>train</code>, <code>test_model</code>  and <code>finetune</code> commands; evaluating and saving the trained model; and start focusing on the explainability task! </p>"},{"location":"#supported-deep-learning-architectures","title":"Supported deep learning architectures","text":"<ul> <li>Chambon2018 model for sleep stage classification ( raw time series as input).</li> <li>TinySleepNet model for sleep stage classification (raw time series as input).</li> <li>SeqSleepNet model for sleep stage classification (time-frequency images as input).</li> <li>SleepTransformer model for sleep stage classification (time-frequency images as input).</li> </ul>"},{"location":"#supported-datasets","title":"Supported datasets","text":""},{"location":"#publicly-available","title":"Publicly Available:","text":"<p>For these datasets PhysioEx will take also to download the data with the <code>preprocess</code> command.</p> <ul> <li>Sleep-EDF(78), The sleep-edf database contains 197 whole-night PolySomnoGraphic sleep recordings, containing EEG, EOG, chin EMG, and event markers.</li> <li>HMC (Haaglanden Medisch Centrum), is a collection of 151 whole-night PSG recordings from 85 men and 66 women, gathered at the Haaglanden Medisch Centrum sleep center. The PSG data includes 4 EEG channels (F4/M1, C4/M1, O2/M1, and C3/M2), two EOG channels (E1/M2 and E2/M2), and one bipolar chin EMG, with all signals sampled at 256 Hz.</li> <li>DCSM (Danish Center for Sleep Medicine), is a collection of 255 randomly selected and fully anonymized overnight lab-based PSG recordings from patients seeking diagnosis for non-specific sleep-related disorders at the DCSM. The PSG setup included EEG, EOG, and EMG channels, all sampled at 256 Hz.</li> </ul>"},{"location":"#nssr-datasets","title":"NSSR Datasets","text":"<p>These datasets can be easily get from the NSSR archive. Once downloaded in your <code>data_folder</code>, place them into the folder <code>data_folder/dataset_name/</code> directory with name <code>dataset_raw</code>. Then you can run the <code>preprocess -d dataset_name -df data_folder</code> command to make the data readable by PhysioEx. </p> <ul> <li>SHHS (Sleep Heart Health Study), is a multi-center cohort study designed to investigate the cardiovascular and other consequences of sleep-disordered breathing. At visit 1, it included 5,793 participants aged 40 years or older. PSG recordings were typically conducted in the subjects' homes by trained and certified technicians. The recording montage included C3/A2 and C4/A1 EEGs sampled at 125 Hz, right and left EOGs sampled at 50 Hz, and a bipolar submental EMG sampled at 125 Hz.</li> <li>MESA (Multi-Ethnic Study of Atherosclerosis), is a multi-center prospective study of 2.237 ethnically diverse men and women aged 45-84 from six communities in the United States. PSGs recordings were obtained using in-home settings including central C4-M1 EEG, bilateral EOG and chin EMG sampled at 256Hz. PSGs were scored by one of 3 MESA certified, registered polysomnologists.</li> <li>MrOS (The Osteoporotic Fractures in Men Study), is a multicenter study comprising 2,911 PSG recordings from men aged 65 years or older, enrolled at six clinical centers. PSG recordings were conducted in home settings and included C3/A2 and C4/A1 EEGs, chin EMG, and left-right EOG, all sampled at 256 Hz.</li> <li>WSC (The Wisconsin Sleep Cohort) A longitudinal study of the causes, consequences, and natural history of sleep disorders using overnight in-laboratory sleep recordings gathered at the University of Wisconsin, United States, with a baseline sample of 1,500 subjects assessed at four-year intervals. The study consists of multiple visits with overnight PSG data acquisition. PSG recordings included C3/M2 EEG, EMG, and left-right EOG, all sampled at 200 Hz.</li> </ul>"},{"location":"#others","title":"Others","text":"<ul> <li>MASS (Montreal Archive of Sleep Studies), is an open-access collaborative database containing laboratory-based PSG recordings. It includes 200 complete nights recorded from 97 men and 103 women, aged 18 to 76 years. All recordings have a sampling frequency of 256 Hz and feature an EEG montage of 4\u201320 channels, along with standard EOG and EMG</li> </ul>"},{"location":"#installation-guidelines","title":"Installation guidelines","text":""},{"location":"#create-a-virtual-environment-optional-but-recommended","title":"Create a Virtual Environment (Optional but Recommended)","text":"<pre><code>$ conda create -n physioex python==3.10\n$ conda activate physioex\n$ conda install pip\n$ pip install --upgrade pip  # On Windows, use `venv\\Scripts\\activate`\n</code></pre>"},{"location":"#install-from-source-recommended","title":"Install from source ( Recommended )","text":"<ol> <li> <p>Clone the Repository: <pre><code>$ git clone https://github.com/guidogagl/physioex.git\n$ cd physioex\n</code></pre></p> </li> <li> <p>Install Dependencies and Package in Development Mode <pre><code>$ pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"#install-via-pip","title":"Install via pip","text":"<ol> <li>Install PhysioEx from PyPI: <pre><code>$ pip install physioex\n</code></pre></li> </ol>"},{"location":"#install-via-pip_1","title":"Install via pip","text":"<ol> <li>Install PhysioEx from PyPI: <pre><code>$ pip install physioex\n</code></pre></li> </ol> <p>Note: the github version of the library is kept updated weekly, the PiPy version may be outdated depending on the last commit of the github version. We recommend to use the github version if possible.  </p>"},{"location":"#cite-us","title":"Cite Us!","text":"<pre><code>@article{10.1088/1361-6579/adaf73,\n    author={Gagliardi, Guido and Alfeo, Luca and Cimino, Mario G C A and Valenza, Gaetano and De Vos, Maarten},\n    title={PhysioEx, a new Python library for explainable sleep staging through deep learning},\n    journal={Physiological Measurement},\n    url={http://iopscience.iop.org/article/10.1088/1361-6579/adaf73},\n    year={2025},\n}\n</code></pre>"},{"location":"examples/latent_space_visualization/","title":"Latent space visualization","text":"<pre><code>import pkg_resources as pkg\npath = pkg.resource_filename(__name__, \"../\")\n\nimport os \nos.chdir( path )\n</code></pre> <pre><code>/tmp/ipykernel_3458100/2665741738.py:2: DeprecationWarning: Use of .. or absolute path in a resource path is not allowed and will raise exceptions in a future release.\n  path = pkg.resource_filename(__name__, \"../\")\n</code></pre> <pre><code>import torch\nfrom pytorch_lightning import LightningModule\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np \nimport seaborn as sns\nimport pandas as pd\n\nfrom physioex.data import SleepPhysionet, TimeDistributedModule\nfrom physioex.train.networks import TinySleepNet, ContrTinySleepNet\n</code></pre> <pre><code>dataset = SleepPhysionet(version = \"2018\", use_cache = True)\ndataset.split(fold = 0)\nsequence_length = 3\nbatch_size = 32\ndatamodule = TimeDistributedModule(dataset = dataset, sequence_lenght = sequence_length, batch_size = batch_size, transform = None, target_transform = None)\n\nccl_model_path = \"models/CCL/tinysleepnet/seqlen=3/SleepPhysionet/2018/fold=0-epoch=15-step=77670-val_acc=0.79.ckpt\"\nscl_model_path = \"models/SCL/tinysleepnet/seqlen=3/SleepPhysionet/2018/fold=0-epoch=18-step=93864-val_acc=0.79.ckpt\"\n</code></pre> <pre><code>\u001b[32m2023-12-23 10:19:36.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mphysioex.data.utils\u001b[0m:\u001b[36mread_cache\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mReading chache from temp/sleep_physionet_2018.pkl\u001b[0m\n</code></pre>"},{"location":"examples/latent_space_visualization/#pca-components-of-the-similarity-model","title":"PCA components of the Similarity Model","text":""},{"location":"examples/latent_space_visualization/#extracting-the-latent-space-projections-from-the-similarity-model","title":"Extracting the latent space projections from the similarity model","text":"<pre><code>model = ContrTinySleepNet.load_from_checkpoint(scl_model_path).eval()\n\ntrain_projections = []\ny_train_true = []\ny_train_pred = []\n\n# scorri tutti i batch del train dataloader \nfor batch in datamodule.train_dataloader():\n    inputs, y_true = batch\n\n    y_train_true.append(y_true)\n\n    projections, y_pred = model(inputs.to(model.device))\n    y_train_pred.append(y_pred.cpu().detach().numpy())\n    train_projections.append(projections.cpu().detach().numpy())\n\n    del projections, y_pred\n\ny_train_true = np.concatenate(y_train_true).reshape(-1)\ntrain_projections = np.concatenate(train_projections).reshape(y_train_true.shape[0], -1)\ny_train_pred = np.concatenate(y_train_pred).reshape(-1, 5)\n\nprint(train_projections.shape)\nprint(y_train_true.shape)\nprint(y_train_pred.shape)\n</code></pre> <pre><code>(489381, 32)\n(489381,)\n(489381, 5)\n</code></pre>"},{"location":"examples/latent_space_visualization/#plotting-the-first-2-pca-components","title":"Plotting the first 2 PCA components","text":"<pre><code>preds = np.argmax(y_train_pred, axis = 1)\ntrue_indx = np.where(y_train_true == preds)[0]\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(train_projections[true_indx, :])\ntrue_positive = y_train_true[true_indx]\n\ndf = pd.DataFrame({\n    'First Principal Component': train_projections[true_indx, 0],\n    'Second Principal Component': train_projections[true_indx, 1],\n    'Group': true_positive\n})\n\nplt.figure(figsize=(8, 6))\n\n# Utilizza seaborn.scatterplot\nsns.scatterplot(data=df, x='First Principal Component', y='Second Principal Component', hue='Group', palette='Set1')\n\nplt.show()\nplt.close()\n</code></pre>"},{"location":"examples/latent_space_visualization/#assessing-the-latent-space-quality-measuring-the-ari","title":"Assessing the latent space quality measuring the ARI","text":"<pre><code>K_values = range(2, 10)\n\n# Lista per memorizzare i risultati ARI\nari_values = []\n\nfor K in K_values:\n    # Esegui K-Means con il valore corrente di K\n    kmeans = KMeans(n_clusters=K, random_state=0).fit(train_projections[true_indx, :])\n\n    # Calcola l'ARI confrontando le assegnazioni di K-Means con i true_positive\n    ari = adjusted_rand_score(true_positive, kmeans.labels_)\n\n    # Memorizza il risultato\n    ari_values.append(ari)\n\n# Crea un DataFrame dai risultati\nresults = pd.DataFrame({\n    'Number of clusters (K)': K_values,\n    'Adjusted Rand Index (ARI)': ari_values\n})\n\n# Plotta i risultati con Seaborn\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=results, x='Number of clusters (K)', y='Adjusted Rand Index (ARI)', marker='o')\nplt.show()\nplt.close()\n</code></pre> <pre><code>/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n</code></pre>"},{"location":"examples/latent_space_visualization/#repeating-the-same-experiment-with-the-standard-model","title":"Repeating the same experiment with the Standard Model","text":"<pre><code>model = TinySleepNet.load_from_checkpoint(ccl_model_path).eval()\n\ntrain_projections = []\ny_train_true = []\ny_train_pred = []\n\n# scorri tutti i batch del train dataloader \nfor batch in datamodule.train_dataloader():\n    inputs, y_true = batch\n\n    y_train_true.append(y_true)\n\n    projections, y_pred = model.encode(inputs.to(model.device))\n    y_train_pred.append(y_pred.cpu().detach().numpy())\n    train_projections.append(projections.cpu().detach().numpy())\n\n    del projections, y_pred\n\ny_train_true = np.concatenate(y_train_true).reshape(-1)\ntrain_projections = np.concatenate(train_projections).reshape(y_train_true.shape[0], -1)\ny_train_pred = np.concatenate(y_train_pred).reshape(-1, 5)\n\nprint(train_projections.shape)\nprint(y_train_true.shape)\nprint(y_train_pred.shape)\n</code></pre> <pre><code>(489381, 128)\n(489381,)\n(489381, 5)\n</code></pre> <pre><code>preds = np.argmax(y_train_pred, axis = 1)\ntrue_indx = np.where(y_train_true == preds)[0]\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(train_projections[true_indx, :])\ntrue_positive = y_train_true[true_indx]\n\ndf = pd.DataFrame({\n    'First Principal Component': train_projections[true_indx, 0],\n    'Second Principal Component': train_projections[true_indx, 1],\n    'Group': true_positive\n})\n\nplt.figure(figsize=(8, 6))\n\n# Utilizza seaborn.scatterplot\nsns.scatterplot(data=df, x='First Principal Component', y='Second Principal Component', hue='Group', palette='Set1')\n\nplt.show()\nplt.close()\n</code></pre> <pre><code>/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/IPython/core/pylabtools.py:152: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n</code></pre> <pre><code># Valori di K da testare\nK_values = range(2, 10)\n\n# Lista per memorizzare i risultati ARI\nari_values = []\n\nfor K in K_values:\n    # Esegui K-Means con il valore corrente di K\n    kmeans = KMeans(n_clusters=K, random_state=0).fit(train_projections[true_indx, :])\n\n    # Calcola l'ARI confrontando le assegnazioni di K-Means con i true_positive\n    ari = adjusted_rand_score(true_positive, kmeans.labels_)\n\n    # Memorizza il risultato\n    ari_values.append(ari)\n\n# Crea un DataFrame dai risultati\nresults = pd.DataFrame({\n    'Number of clusters (K)': K_values,\n    'Adjusted Rand Index (ARI)': ari_values\n})\n\n# Plotta i risultati con Seaborn\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=results, x='Number of clusters (K)', y='Adjusted Rand Index (ARI)', marker='o')\nplt.show()\nplt.close()\n</code></pre> <pre><code>/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n/home/guido/miniconda3/envs/physioex/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"pages/contribute/","title":"Contributing to PhysioEx","text":"<p>Welcome to the PhysioEx project! We appreciate your interest in contributing. </p>"},{"location":"pages/contribute/#ways-to-contribute","title":"Ways to contribute","text":"<p>Numerous avenues exist for contributing to the PhysioEx project, with the most prevalent being the provision of code or enhancements to the documentation. The refinement of the documentation is equally as vital as the augmentation of the library itself.</p> <p>Should you discover an error in the documentation, or have implemented improvements, we encourage you to post a new message on the GitHub discussion board or, ideally, submit a GitHub pull request.</p> <p>An additional method of contribution involves reporting any issues you encounter, and endorsing issues reported by others that are pertinent to you by giving them a \u201cthumbs up\u201d. Your support in promoting the project is also beneficial: mention the project in your blog posts and articles, provide a link to it from your website, or simply give it a star to indicate your usage of it.</p>"},{"location":"pages/contribute/#adviced-contributing-fields","title":"Adviced contributing fields","text":"<ol> <li>Adding support for a new physiological signal dataset (e.g. checking PhysioNet or MOABB)</li> <li>Adding support for a new deep learning architecture for physiological signal analysis.</li> <li>Proposing a novel Explainable AI algorithm.</li> </ol> <p>Consider also this other very important fields of contribution:</p> <ul> <li>Documentation improving, note: check the official MkDocs Material doc. as reference.</li> <li>Code improving and issues solving.</li> </ul>"},{"location":"pages/contribute/#how-to-contribute","title":"How to Contribute","text":"<p>We welcome contributions from everyone. PhysioEx is a GitHub hosted libray, to know more about GitHub collaborative devolpment check their ufficial doc.To get started, follow these steps:</p> <ol> <li>Fork the Repository</li> </ol> <p>Fork this repository to your GitHub account by clicking on the 'Fork' button on the top right of the page. This creates a copy of the repository in your account.</p> <ol> <li>Clone your fork of the physioex repo from your GitHub account to your local disk:</li> </ol> <pre><code>    $ git clone git@github.com:your-username/physioex.git \n    $ cd physioex \n</code></pre> <p>Replace <code>your-username</code> with your GitHub username.</p> <ol> <li> <p>Make sure to have anaconda or miniconda correctly installed in your machine, then start installing a new virtual enviroment <pre><code>    $ conda create -n myenv python==3.10\n</code></pre></p> </li> <li> <p>Now jump into the enviroment and upgrade pip <pre><code>    $ conda activate myenv\n    $ conda install pip\n    $ pip install --upgrade pip\n</code></pre></p> </li> <li> <p>To install PhysioEx in development mode run: <pre><code>    $ pip install -e .\n</code></pre></p> </li> <li> <p>Now you need to keep your fork and the original physioex repo in sync creating an upstream: <pre><code>    $ git remote add upstream https://github.com/guidogagl/physioex.git\n</code></pre></p> </li> </ol> <p>Warning</p> <p>To check that the upstream is correctly setted up, run: <pre><code>    $ git remote -v\n</code></pre> And check that the output resembles: <pre><code>    &gt; origin    https://github.com/your-username/physioex.git (fetch)\n\n    &gt; origin    https://github.com/your-username/physioex.git (push)\n\n    &gt; upstream  https://github.com/guidogagl/physioex.git (fetch)\n    &gt; upstream  https://github.com/guidogagl/physioex.git (push)\n</code></pre></p> <p>Tip</p> <p>To keep your fork repo updated check the official doc</p> <ol> <li>Write your code &amp; documentation and when it's ready submit a Pull Request! For a step-by-step guide on how to submit a PR check the GitHub official documentation</li> </ol>"},{"location":"pages/data/","title":"Data Module","text":"<p>The <code>physioex.data</code> module provides the API to read the data from the disk once the raw datasets have been processed by the <code>Preprocess</code> module. It consists of two classes: </p> <ul> <li><code>physioex.data.PhysioExDataset</code> which serialize the disk processed version of the dataset into a <code>PyTorch Dataset</code></li> <li><code>physioex.data.PhysioExDataModule</code> which transforms the datasets to <code>PyTorch DataLoaders</code> ready for training. </li> </ul>"},{"location":"pages/data/#example-of-usage","title":"Example of Usage","text":""},{"location":"pages/data/#physioexdataset","title":"PhysioExDataset","text":"<p>The <code>PhysioExDataset</code> class is automatically handled by the <code>PhysioExDataModule</code> class when you need to use it for training or testing purposes. In most of the cases you don't need to interact with the <code>PhysioExDataset</code> class.</p> <p>The class is instead really helpfull when you need to visualize your data, or you need to get some samples of your data to provide them as input to Explainable AI algorithms.</p> <p>In these cases you need to instantiate a <code>PhysioExDataset</code>:</p> <pre><code>from physioex.data import PhysioExDataset\n\ndata = PhysioExDataset(\n    datasets = [\"hmc\"], # you can read different datasets merged together in this way\n    preprocessing = \"raw\",  \n    selected_channels = [\"EEG\", \"EOG\", \"EMG\"],     \n    data_folder = \"/your/data/path/\",\n)\n\n# you can now access any sequence of epochs in the dataset\nsignal, label = data[0]\n\nsignal.shape # will be [21 (default sequence lenght), 3, 3000]\nlabel.shape # will be [21]\n</code></pre> <p>Then you can use a python plotting library to plot visualize the data</p> <p>Example</p> <pre><code>import seaborn as sns\nimport numpy as np \n\nhypnogram = np.ones((21, 3000)) * label.numpy().reshape(-1, 1)\n\n# plot a subfigure with one column for each element of the sequence (21)\nfig, ax = plt.subplots(4, 1, figsize = (21, 8), sharex=\"col\", sharey=\"row\")\n\nhypnogram = hypnogram.reshape( -1 )\nsignals = signal.numpy().transpose(1, 0, 2).reshape(3, -1)\n\n# set tytle for each subplot\nsns.lineplot( x = range(3000*21), y = hypnogram, ax = ax[0], color = \"blue\")\n# then the channels:\nsns.lineplot( x = range(3000*21), y = signals[ 0], ax = ax[1], color = \"red\")\nsns.lineplot( x = range(3000*21), y = signals[ 1], ax = ax[2], color = \"green\")\nsns.lineplot( x = range(3000*21), y = signals[ 2], ax = ax[3], color = \"purple\")    \n\n# check the examples notebook \"visualize_data.ipynb\" to see how to customize the plot properly\n\nplt.tight_layout()\n</code></pre> <p></p>"},{"location":"pages/data/#physioexdatamodule","title":"PhysioExDataModule","text":"<p>The <code>PhysioExDataModule</code> class is designed to transform datasets into <code>PyTorch DataLoaders</code> ready for training. It handles the batching, shuffling, and splitting of the data into training, validation, and test sets.</p> <p>To use the <code>PhysioExDataModule</code>, you need to instantiate it with the required parameters:</p> <pre><code>from physioex.data import PhysioExDataModule\n\ndatamodule = PhysioExDataModule(\n    datasets=[\"hmc\", \"mass\"],  # list of datasets to be used\n    batch_size=64,             # batch size for the DataLoader\n    preprocessing=\"raw\",       # preprocessing method\n    selected_channels=[\"EEG\", \"EOG\", \"EMG\"],  # channels to be selected\n    sequence_length=21,        # length of the sequence\n    data_folder=\"/your/data/path/\",  # path to the data folder\n)\n\n# get the DataLoaders\ntrain_loader = datamodule.train_dataloader()\nval_loader = datamodule.val_dataloader()\ntest_loader = datamodule.test_dataloader()\n</code></pre> <p>PhysiEx is built on <code>pytorch_lightning</code> for model training and testig, hence you can use <code>PhysioExDataModule</code> in combination with <code>pl.Trainer</code></p> <pre><code>from pytorch_lightning import Trainer\n\nmodel = SomePytorchModel()\n\ntrainer = Trainer(\n    devices=\"auto\"\n    max_epochs=10,\n    deterministic=True,\n)\n\n# setup the model in training mode if needed\nmodel = model.train()\n# Start training\ntrainer.fit(model, datamodule=datamodule)\nresults = trainer.test( model, datamodule = datamodule)\n</code></pre>"},{"location":"pages/data/#documentation","title":"Documentation","text":"<p><code>PhysioExDataset</code> </p> <p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset class for handling physiological data from multiple datasets.</p> <p>Attributes:</p> Name Type Description <code>datasets</code> <code>List[str]</code> <p>List of dataset names.</p> <code>L</code> <code>int</code> <p>Sequence length.</p> <code>channels_index</code> <code>List[int]</code> <p>Indices of selected channels.</p> <code>readers</code> <code>List[DataReader]</code> <p>List of DataReader objects for each dataset.</p> <code>tables</code> <code>List[DataFrame]</code> <p>List of data tables for each dataset.</p> <code>dataset_idx</code> <code>ndarray</code> <p>Array indicating the dataset index for each sample.</p> <code>target_transform</code> <code>Callable</code> <p>Optional transform to be applied to the target.</p> <code>len</code> <code>int</code> <p>Total number of samples across all datasets.</p> <p>Methods:</p> Name Description <code>__len__</code> <p>Returns the total number of samples.</p> <code>split</code> <p>int = -1, dataset_idx: int = -1): Splits the data into train, validation, and test sets.</p> <code>get_num_folds</code> <p>Returns the minimum number of folds across all datasets.</p> <code>__getitem__</code> <p>Returns the input and target for a given index.</p> <code>get_sets</code> <p>Returns the indices for the train, validation, and test sets.</p> <p><code>PhysioExDataModule</code> </p> <p>               Bases: <code>LightningDataModule</code></p> <p>A PyTorch Lightning DataModule for handling physiological data from multiple datasets.</p> <p>Attributes:</p> Name Type Description <code>datasets_id</code> <code>List[str]</code> <p>List of dataset names.</p> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading.</p> <code>dataset</code> <code>PhysioExDataset</code> <p>The dataset object.</p> <code>batch_size</code> <code>int</code> <p>Batch size for the DataLoader.</p> <code>hpc</code> <code>bool</code> <p>Flag indicating whether to use high-performance computing.</p> <code>train_dataset</code> <code>Union[PhysioExDataset, Subset]</code> <p>Training dataset.</p> <code>valid_dataset</code> <code>Union[PhysioExDataset, Subset]</code> <p>Validation dataset.</p> <code>test_dataset</code> <code>Union[PhysioExDataset, Subset]</code> <p>Test dataset.</p> <code>train_sampler</code> <code>Union[SubsetRandomSampler, Subset]</code> <p>Sampler for the training dataset.</p> <code>valid_sampler</code> <code>Union[SubsetRandomSampler, Subset]</code> <p>Sampler for the validation dataset.</p> <code>test_sampler</code> <code>Union[SubsetRandomSampler, Subset]</code> <p>Sampler for the test dataset.</p> <p>Methods:</p> Name Description <code>setup</code> <p>str): Sets up the datasets for different stages.</p> <code>train_dataloader</code> <p>Returns the DataLoader for the training dataset.</p> <code>val_dataloader</code> <p>Returns the DataLoader for the validation dataset.</p> <code>test_dataloader</code> <p>Returns the DataLoader for the test dataset.</p>"},{"location":"pages/data/#data.PhysioExDataset.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PhysioExDataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>List[str]</code> <p>List of dataset names.</p> required <code>data_folder</code> <code>str</code> <p>Path to the folder containing the data.</p> required <code>preprocessing</code> <code>str</code> <p>Type of preprocessing to apply. Defaults to \"raw\".</p> required <code>selected_channels</code> <code>List[int]</code> <p>List of selected channels. Defaults to [\"EEG\"].</p> required <code>sequence_length</code> <code>int</code> <p>Length of the sequence. Defaults to 21.</p> required <code>target_transform</code> <code>Callable</code> <p>Optional transform to be applied to the target. Defaults to None.</p> required <code>hpc</code> <code>bool</code> <p>Flag indicating whether to use high-performance computing. Defaults to False.</p> required <code>indexed_channels</code> <code>List[int]</code> <p>List of indexed channels. Defaults to [\"EEG\", \"EOG\", \"EMG\", \"ECG\"]. If you used a custom Preprocessor and you saved your signal channels in a different order, you should provide the correct order here. In any other case ignore this parameter.</p> required"},{"location":"pages/data/#data.PhysioExDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Returns the input and target sequence for a given index.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to retrieve.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>Input and target for the given index.</p>"},{"location":"pages/data/#data.PhysioExDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of sequences of epochs across all the datasets.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Total number of sequences.</p>"},{"location":"pages/data/#data.PhysioExDataset.split","title":"<code>split(fold=-1, dataset_idx=-1)</code>","text":"<p>Splits the data into train, validation, and test sets.</p> <p>if fold is -1, and dataset_idx is -1 : set the split to a random fold for each dataset if fold is -1, and dataset_idx is not -1 : set the split to a random fold for the selected dataset if fold is not -1, and dataset_idx is -1 : set the split to the selected fold for each dataset if fold is not -1, and dataset_idx is not -1 : set the split to the selected fold for the selected dataset</p> <p>Parameters:</p> Name Type Description Default <code>fold</code> <code>int</code> <p>Fold number to use for splitting. Defaults to -1.</p> <code>-1</code> <code>dataset_idx</code> <code>int</code> <p>Index of the dataset to split. Defaults to -1.</p> <code>-1</code>"},{"location":"pages/data/#data.PhysioExDataset.get_num_folds","title":"<code>get_num_folds()</code>","text":"<p>Returns the minimum number of folds across all datasets.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Minimum number of folds.</p>"},{"location":"pages/data/#data.PhysioExDataset.get_sets","title":"<code>get_sets()</code>","text":"<p>Returns the indices for the train, validation, and test sets.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Indices for the train, validation, and test sets.</p>"},{"location":"pages/data/#data.PhysioExDataModule.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the PhysioExDataModule.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>List[str]</code> <p>List of dataset names.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for the DataLoader. Defaults to 32.</p> required <code>preprocessing</code> <code>str</code> <p>Type of preprocessing to apply. Defaults to \"raw\".</p> required <code>selected_channels</code> <code>List[int]</code> <p>List of selected channels. Defaults to [\"EEG\"].</p> required <code>sequence_length</code> <code>int</code> <p>Length of the sequence. Defaults to 21.</p> required <code>target_transform</code> <code>Callable</code> <p>Optional transform to be applied to the target. Defaults to None.</p> required <code>folds</code> <code>Union[int, List[int]]</code> <p>Fold number(s) for splitting the data. Defaults to -1.</p> required <code>data_folder</code> <code>str</code> <p>Path to the folder containing the data. Defaults to None.</p> required <code>num_nodes</code> <code>int</code> <p>Number of nodes for distributed training. Defaults to 1.</p> required <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. Defaults to os.cpu_count().</p> required"},{"location":"pages/data/#data.PhysioExDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the DataLoader for the training dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <p>DataLoader for the training dataset.</p>"},{"location":"pages/data/#data.PhysioExDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the DataLoader for the test dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <p>DataLoader for the test dataset.</p>"},{"location":"pages/preprocess/","title":"Preprocess Module","text":"<p>The preprocess module implements a standard API to import custom or benchmark sleep staging datasets into PhysioEx and let them serializable into a <code>PhysioExDataset</code>. This functionality is provided by the <code>physioex.preprocess.preprocessor.Preprocessor</code> class.</p> <p>The <code>Preprocessor</code> class is designed to facilitate the preprocessing of physiological datasets. This class provides methods for downloading datasets, reading subject records, applying preprocessing functions, and organizing the data into a structured format suitable for machine learning tasks.</p>"},{"location":"pages/preprocess/#example-usage","title":"Example Usage","text":"<p>If you want to preprocess a dataset using a preprocessing function</p> <pre><code># Preprocessor is an abstract class, you need an implementation of it to use it. \n# This can be you own defined Preprocessor or one of the available into PhysioEx\nfrom physioex.preprocess.hmc import HMCPreprocessor  \n\n# Define preprocessing functions, it's just a Callable method on each signal\nfrom physioex.preprocess.utils.signal import  xsleepnet_preprocessing\n\n# Initialize Preprocessor\npreprocessor = HMCPreprocessor(\n    preprocessors_name = [\"xsleepnet\"], # the name of the preprocessor\n    preprocessors = [xsleepnet_preprocessing], # the callable preprocessing method\n    preprocessor_shape = [[4, 29, 129]], # the output of the signal after preprocessing, \n                                         # the first element (4) depends on the number of \n                                         # channels available in your system. In HMC they are 4.\n    data_folder = \"/your/data/path/\"\n)\n\n# Run preprocessing\npreprocessor.run()\n\n# at this point you can read the dataset from the disk using PhysioExDataset\n\nfrom physioex.data import PhysioExDataset\n\ndata = PhysioExDataset(\n    datasets = [\"hmc\"],\n    preprocessing = \"xsleepnet\",    # can be \"raw\" also because the Preprocessor \n                                    # will always save also the raw data\n    selected_channels = [\"EEG\", \"EOG\", \"EMG\", \"ECG\"], # in case you want to read \n                                                      # all the channels available\n    data_folder = \"/your/data/path/\",\n)\n\n# you can now access any sequence of epochs in the dataset\n\nsignal, label = data[0]\n\nsignal.shape # will be [21 (default sequence lenght), 4, 29, 129]\nlabel.shape # will be [21]\n</code></pre>"},{"location":"pages/preprocess/#cli","title":"CLI","text":"<p>If you want to use the standard PhysioEx implementation of the preprocessor, which will save the data in the raw format and in the format proposed by XSleepNet, you can use the CLI tool, once the library is istalled just type:</p> <pre><code>preprocess --dataset hmc --data_folder  \"/your/data/path/\"\n</code></pre> <p>The list of available dataset is:</p> <ul> <li>SHHS (Sleep Heart Health Study)</li> <li>MASS (Montreal Archive of Sleep Studies)</li> <li>MESA (Multi-Ethnic Study of Atherosclerosis)</li> <li>MrOS (The Osteoporotic Fractures in Men Study)</li> <li>HMC (Haaglanden Medisch Centrum)</li> <li>DCSM (Danish Center for Sleep Medicine)</li> </ul> <p>Note that for the HMC and DCSM dataset the library will take care to download the dataset if not available into <code>/your/data/path/</code>.</p> <p><code>preprocess</code> </p> <p>Preprocessing script for preparing datasets.</p> <p>This script allows you to preprocess datasets for training and testing models.</p> Usage <p><code>$ preprocess [PARAMS]</code> You can use the <code>preprocess -h --help</code> command to access the command documentation.</p> <p>Parameters:</p> Name Type Description Default <code>--dataset</code> <code>str</code> <p>The name of the dataset to preprocess. Defaults to \"hmc\". Note: The dataset name should be one of the supported datasets (e.g., \"hmc\", \"mass\", \"shhs\", \"mesa\", \"mros\", \"dcsm\"). If a custom dataset is used use the <code>preprocessor</code> argument.</p> required <code>--data_folder</code> <code>str</code> <p>The absolute path of the directory where the physioex dataset are stored, if None the home directory is used. Defaults to None. Note: Provide the path to the directory containing the datasets.</p> required <code>--preprocessor</code> <code>str</code> <p>The name of the preprocessor in case of a custom Preprocessor. Defaults to None. Note: The preprocessor should extend <code>physioex.preprocess.proprocessor:Preprocessor</code> and be passed as a string in the format <code>path.to.preprocessor.module:PreprocessorClass</code>.</p> required <code>--config</code> <code>str</code> <p>Specify the path to the configuration .yaml file where to store the options to preprocess the dataset with. Defaults to None. Note: The configuration file can override command line arguments. You can specify also the preprocessor_kwargs in the configuration file.</p> required Example <p><pre><code>$ preprocess --dataset mass --data_folder /path/to/datasets\n</code></pre> This command preprocesses the <code>mass</code> dataset using the <code>MASSPreprocessor</code> preprocessor.</p> <p>For HMC and DCSM datasets, PhysioEx will automatically download the datasets. The other datasets needs to be obtained first, most of them are easily accessible from sleepdata.org.</p> <p>The SHHS and MASS dataset needs to be further processed after download with the script in:</p> <pre><code>- MASS: https://github.com/pquochuy/xsleepnet/tree/master/mass\n- SHHS: https://github.com/pquochuy/SleepTransformer/tree/main/shhs\n</code></pre> <p>Once you obtain the mat/ folder using this processing scripts place them into data_folder/dataset_name/mat/ and run the preprocess command.</p> <p>The command can use a .yaml configuration file to specify the preprocessor_kwargs:</p> <pre><code>    dataset: null\n    data_folder : /path/to/your/data\n    preprocessor : physioex.preprocess.hmc:HMCPreprocessor # can be also your custom preprocessor\n    preprocessor_kwargs:\n        # signal_shape: [4, 3000]\n        preprocessors_name:\n            - \"your_preprocessor\"\n            - \"xsleepnet\"\n        preprocessors:\n            - path.to.your.module:your_preprocessor\n            - physioex.preprocess.utils.signal:xsleepnet_preprocessing\n        preprocessors_shape:\n            - [4, 3000]\n            - [4, 3000]\n            - [4, 29, 129]\n</code></pre> Notes <ul> <li>Ensure that the datasets are properly formatted and stored in the specified data folder using the preprocess script.</li> <li>The configuration file, if provided, should be in YAML format and contain valid key-value pairs for the script options.</li> </ul>"},{"location":"pages/preprocess/#extending-the-preprocessor-class","title":"Extending the Preprocessor Class","text":"<p>To build you own defined preprocessor you should extend the Preprocessor class.</p> <p>For instance lets consider how we extended the Preprocesor class to preprocess the HMC dataset <pre><code>from physioex.preprocess.preprocessor import Preprocessor\n\nclass HMCPreprocessor(Preprocessor):\n    def __init__(self, \n            preprocessors_name: List[str] = [\"xsleepnet\"],\n            preprocessors = [xsleepnet_preprocessing],\n            preprocessor_shape = [[4, 29, 129]],\n            data_folder: str = None\n            ):\n\n        # calls the Preprocessor constructor, required at the end of your custom setup\n        super().__init__(\n            dataset_name=\"hmc\",     # this is the name of the dataset PhysioEx will use \n                                    # as PhysioExDataset( dataset=[dataset_name] )\n            signal_shape=[4, 3000], # PhysioEx reads sleep epochs of 30 seconds sampled at 100Hz. \n                                    # 4 Is the total amount of channel available in the dataset\n            preprocessors_name=preprocessors_name,\n            preprocessors=preprocessors,\n            preprocessors_shape=preprocessor_shape,\n            data_folder=data_folder,\n        )\n\n    @logger.catch\n    def download_dataset(self) -&gt; None: \n        # download the dataset into the data_folder/download/hmc_dataset.zip\n        # extract the zip \n\n        download_dir = os.path.join(self.dataset_folder, \"download\")\n\n        if not os.path.exists(download_dir):\n            os.makedirs(download_dir, exist_ok=True)\n\n            zip_file = os.path.join(self.dataset_folder, \"hmc_dataset.zip\")\n\n            if not os.path.exists(zip_file):\n                download_file(\n                    \"https://physionet.org/static/published-projects/hmc-sleep-staging/haaglanden-medisch-centrum-sleep-staging-database-1.1.zip\",\n                    zip_file,\n                )\n\n            extract_large_zip(zip_file, download_dir)\n\n    @logger.catch\n    def get_subjects_records(self) -&gt; List[str]:\n        # read the RECORDS file into the extracted directory and returns the list of the available records\n        subjects_dir = os.path.join(\n            self.dataset_folder,\n            \"download\",\n            \"haaglanden-medisch-centrum-sleep-staging-database-1.1\",\n        )\n\n        records_file = os.path.join(subjects_dir, \"RECORDS\")\n\n        with open(records_file, \"r\") as file:\n            records = file.readlines()\n\n        records = [record.rstrip(\"\\n\") for record in records]\n\n        return records\n\n    @logger.catch\n    def read_subject_record(self, record: str) -&gt; Tuple[np.array, np.array]:\n        # read each RECORD ( which is an .edf file ) and return its signal and labels\n        return read_edf(\n            os.path.join(\n                self.dataset_folder,\n                \"download\",\n                \"haaglanden-medisch-centrum-sleep-staging-database-1.1\",\n                record,\n            )\n        )\n</code></pre></p> <p>Here there is the pseudocode for a possible implementation of the read_edf method using <code>pyedflib</code></p> <pre><code># an example of a read_edf method\n# Note: if you use a dataset from NSRR you can directly use \n# physioex.preprocess.utils.sleepdata:process_sleepdata_file\n#       \n# if you want to read different channels you can choose them here\n#\nstages_map = [  # used to map each stage in the annotation file\n                # to an identifier ( the index of the list )\n    \"Sleep stage W\",\n    \"Sleep stage N1\",\n    \"Sleep stage N2\",\n    \"Sleep stage N3\",\n    \"Sleep stage R\",\n]\n\nfs = 256    # sampling frequency of the signal readed\n            # can be readed directly from the .edf file if you have\n            # different sampling frequencies for different channels\n\n# the channels you want to read and preprocess in your dataset\nAVAILABLE_CHANNELS = [\"EEG C3-M2\", \"EOG\", \"EMG\"]\n\ndef read_edf(file_path):\n\n    # read the annotatations\n    # Note: tipycally record and annotation files have the same name.\n    #       a best practice could be to store them as:\n    #       filepath_stages.edf\n    #       filepath_signal.edf\n    #       in the same directory, \n    #       if this is not the case in your dataset consider returning a tuple\n    #       in the get_subjects_records method : \n    #       file_path = ( signal_path, annotation_path )\n\n    stages_path = file_path + \"_stages.edf\" \n    signal_path = file_path + \"_signal.edf\"\n\n    f = pyedflib.EdfReader(stages_path)\n    _, _, annt = f.readAnnotations()\n    f._close()\n\n    # convert the annotations from string to the index of stages_map\n    stages = []\n    for a in annt:\n        if a in stages_map:\n            stages.append(stages_map.index(a))\n\n    # convert it to a numpy array \n    stages = np.reshape(np.array(stages).astype(int), (-1))\n\n    # read the signal\n    f = pyedflib.EdfReader(signal_path)\n    buffer = []\n    for indx, modality in enumerate(AVAILABLE_CHANNELS):\n\n        signal = f.readSignal( modality ).reshape( -1 )\n\n        # filtering\n        # pass band the signal between 0.3 and 40 Hz\n        # you can use physioex.preprocess.utils.signal:bandpass_filter\n        if modality != \"EMG\":\n            signal = bandpass_filter(signal, 0.3, 40, fs)\n        else:\n            # if EMG signal filter at 10Hz\n            b_band = firwin(101, 10, pass_zero=False, fs=fs)\n            signal = filtfilt(b_band, 1, signal)\n\n        # resampling\n        # 100 Hz * 30 sec * num_epochs ( annotations.shape[0] )\n        # you can use scipy.signal.resample\n        signal = resample(signal, num= 30 * 100 * annotations.shape[0])\n\n        # windowing\n        signal = signal.reshape(-1, 3000)\n        buffer.append(signal)\n    f._close()\n\n    buffer = np.array(buffer) # shape is len(AVAILABLE_CHANNELS), num_epochs, 3000\n    signal = np.transpose(buffer, (1, 0, 2)) #  num_epochs, len(AVAILABLE_CHANNELS), 3000\n    del buffer\n\n    # now you should check if Wake is the biggest class for your subject\n    count_stage = np.bincount(stages)\n    if count_stage[0] &gt; max(count_stage[1:]):  # Wake is the biggest class\n        second_largest = max(count_stage[1:])\n\n        W_ind = stages == 0  # W indices\n        last_evening_W_index = np.where(np.diff(W_ind) != 0)[0][0] + 1\n        if stages[0] == 0:  # only true if the first epoch is W\n            num_evening_W = last_evening_W_index\n        else:\n            num_evening_W = 0\n\n        first_morning_W_index = np.where(np.diff(W_ind) != 0)[0][-1] + 1\n        num_morning_W = len(stages) - first_morning_W_index + 1\n\n        nb_pre_post_sleep_wake_eps = num_evening_W + num_morning_W\n        if nb_pre_post_sleep_wake_eps &gt; second_largest:\n            total_W_to_remove = nb_pre_post_sleep_wake_eps - second_largest\n            if num_evening_W &gt; total_W_to_remove:\n                stages = stages[total_W_to_remove:]\n                signal = signal[total_W_to_remove:]\n            else:\n                evening_W_to_remove = num_evening_W\n                morning_W_to_remove = total_W_to_remove - evening_W_to_remove\n                stages = stages[evening_W_to_remove : len(stages) - morning_W_to_remove]\n                signal = signal[evening_W_to_remove : len(signal) - morning_W_to_remove]\n\n    return signal, stages\n</code></pre>"},{"location":"pages/preprocess/#documentation","title":"Documentation","text":"<p>The list of the methods that the user need to reimplement to extend the Preprocessor class is:</p> <p><code>Preprocessor</code> </p>"},{"location":"pages/preprocess/#preprocessor.Preprocessor.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the Preprocessor class.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>The name of the dataset to be processed.</p> required <code>signal_shape</code> <code>List[int]</code> <p>A list containing two elements representing the number of channels and the number of timestamps in the signal.</p> required <code>preprocessors_name</code> <code>List[str]</code> <p>A list of names for the preprocessing functions.</p> required <code>preprocessors</code> <code>List[Callable]</code> <p>A list of callable preprocessing functions to be applied to the signals.</p> required <code>preprocessors_shape</code> <code>List[List[int]]</code> <p>A list of shapes corresponding to the output of each preprocessing function.</p> required <code>data_folder</code> <code>str</code> <p>The folder where the dataset is stored. If None, the default data folder is used.</p> required"},{"location":"pages/preprocess/#preprocessor.Preprocessor.read_subject_record","title":"<code>read_subject_record()</code>","text":"<p>Reads a subject's record and returns a tuple containing the signal and labels.</p> <p>(Required) Method should be provided by the user.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>str</code> <p>The path to the subject's record.</p> required <p>Returns:</p> Type Description <p>Tuple[np.array, np.array]: A tuple containing the signal and labels with shapes [n_windows, n_channels, n_timestamps] and [n_windows], respectively. If the record should be skipped, the function should return None, None.</p>"},{"location":"pages/preprocess/#preprocessor.Preprocessor.download_dataset","title":"<code>download_dataset()</code>","text":"<p>Downloads the dataset if it is not already present on disk.</p> <p>(Optional) Method to be implemented by the user.</p>"},{"location":"pages/preprocess/#preprocessor.Preprocessor.customize_table","title":"<code>customize_table()</code>","text":"<p>Customizes the dataset table before saving it.</p> <p>(Optional) Method to be provided by the user.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>DataFrame</code> <p>The dataset table to be customized.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: The customized dataset table.</p>"},{"location":"pages/preprocess/#preprocessor.Preprocessor.get_sets","title":"<code>get_sets()</code>","text":"<p>Returns the train, validation, and test subjects.</p> <p>(Optional) Method to be provided by the user. By default, the method splits the subjects randomly with 70% for training, 15% for validation, and 15% for testing.</p> <p>Returns:</p> Type Description <p>Tuple[List, List, List]: A tuple containing the train, validation, and test subjects.</p>"},{"location":"pages/unavailable/","title":"Ups! Sorry this page is currently in development. :C","text":""},{"location":"pages/train/cli/","title":"Command-Line-Interface of the Train Module","text":"<p>PhysioEx provides a fast and customizable way to train, evaluate and save state-of-the-art models for different physiological signal analysis tasks with different physiological signal datasets. This functionality is provided by the <code>train</code>, <code>test_model</code> and <code>finetune</code> commands provided by this repository.</p> <p><code>train</code> CLI </p> <p>Training script for training and testing a model.</p> <p>This script allows you to train a model using specified configurations and parameters.</p> Usage <p><code>$ train [PARAMS]</code>  You can use the <code>train -h, --help</code> command to access the command documentation.</p> <p>Parameters:</p> Name Type Description Default <code>--model</code> <code>str</code> <p>Specify the model to train, can be a yaml file if the model is not registered. Defaults to \"chambon2018\". If a yaml file is provided, it should contain the model configuration details.</p> required <code>--checkpoint_dir</code> <code>str</code> <p>Specify where to save the checkpoint. Defaults to None. Note: Provide the path to the directory where the model checkpoints will be saved.</p> required <code>--datasets</code> <code>list</code> <p>Specify the datasets list to train the model on. Defaults to ['mass']. Note: Provide a list of dataset names to be used for training.</p> required <code>--selected_channels</code> <code>list</code> <p>Specify the channels to train the model. Defaults to ['EEG']. Note: Channels refer to the data modalities (e.g., EEG, EOG) used for training.</p> required <code>--sequence_length</code> <code>int</code> <p>Specify the sequence length for the model. Defaults to 21. Note: Sequence length refers to the number of time steps in each input sequence.</p> required <code>--loss</code> <code>str</code> <p>Specify the loss function to use. Defaults to \"cel\". Note: The loss function determines how the model's performance is measured during training.</p> required <code>--max_epoch</code> <code>int</code> <p>Specify the maximum number of epochs for training. Defaults to 20. Note: An epoch is one complete pass through the training dataset.</p> required <code>--num_validations</code> <code>int</code> <p>Specify the number of validations steps to be done in each epoch. Defaults to 10. Note: Validation steps are used to evaluate the model's performance on a validation set during training.</p> required <code>--batch_size</code> <code>int</code> <p>Specify the batch size for training. Defaults to 32. Note: Batch size refers to the number of samples processed before the model's weights are updated.</p> required <code>--data_folder</code> <code>str</code> <p>The absolute path of the directory where the physioex dataset are stored, if None the home directory is used. Defaults to None. Note: Provide the path to the directory containing the datasets.</p> required <code>--test</code> <code>bool</code> <p>Test the model after training. Defaults to False. Note: If specified, the model will be tested on the validation set after training.</p> required <code>--aggregate</code> <code>bool</code> <p>Aggregate the results of the test. Defaults to False. Note: If specified, the test results will be aggregated across multiple datasets.</p> required <code>--hpc</code> <code>bool</code> <p>Using high performance computing setups or not, need to be called when datasets have been compressed into .h5 format with the compress_datasets command. Defaults to False. Note: Use this option if you are running the script on a high-performance computing cluster.</p> required <code>--num_nodes</code> <code>int</code> <p>Specify the number of nodes to be used for distributed training, only used when hpc is True. Defaults to 1. Note: In slurm this value needs to be coherent with '--ntasks-per-node' or 'ppn' in torque. This option is relevant for distributed training setups.</p> required <code>--config</code> <code>str</code> <p>Specify the path to the configuration file where to store the options to train the model with. Defaults to None. Note: The configuration file can override command line arguments.</p> required Example <p>The basic usage is as follows:</p> <pre><code>train --model chambon2018 --datasets mass --checkpoint_dir ./checkpoints --max_epoch 20 --batch_size 32\n</code></pre> <p>or you can specify a yaml file containing the configuration details:</p> .yamlbash <pre><code>model_package: physioex.train.networks.seqsleepnet\nmodel_class: SeqSleepNet\nmodule_config:\n    seq_len: 21\n    in_channels: 1\n    loss_call: cel # in this case you can pass the loss call as a string\n    loss_params: {}\npreprocessing: xsleepnet\ntarget_transform: get_mid_label\n# check the train documentaion for more details\n</code></pre> <pre><code>train --model my_model_config.yaml --datasets mass hmc --checkpoint_dir ./checkpoints --max_epoch 20 --batch_size 32\n</code></pre> Notes <ul> <li>Ensure that the datasets are properly formatted and stored in the specified data folder using the preprocess script.</li> <li>The script supports both single-node and multi-node training setups.</li> <li>The configuration file, if provided, should be in YAML format and contain valid key-value pairs for the script options.</li> </ul> <p><code>test</code> CLI </p> <p>Testing script for evaluating a model.</p> <p>This script allows you to test a pre-trained model using specified configurations and parameters.</p> Usage <p><code>$ test_model [PARAMS]</code>  You can use the <code>test_model -h, --help</code> command to access the command documentation.</p> <p>Parameters:</p> Name Type Description Default <code>`--model`</code> <code>str</code> <p>Specify the model to test, can be a yaml file if the model is not registered. Defaults to \"chambon2018\". If a yaml file is provided, it should contain the model configuration details.</p> required <code>`--checkpoint_path`</code> <code>str</code> <p>Specify the model checkpoint. Defaults to None. Note: Provide the path to a specific checkpoint file to load the model state.</p> required <code>`--datasets`</code> <code>list</code> <p>Specify the datasets list to test the model on. Defaults to ['mass']. Note: Provide a list of dataset names to be used for testing.</p> required <code>`--selected_channels`</code> <code>list</code> <p>Specify the channels to test the model. Defaults to ['EEG']. Note: Channels refer to the data modalities (e.g., EEG, EOG) used for testing.</p> required <code>`--sequence_length`</code> <code>int</code> <p>Specify the sequence length for the model. Defaults to 21. Note: Sequence length refers to the number of time steps in each input sequence.</p> required <code>`--loss`</code> <code>str</code> <p>Specify the loss function to use. Defaults to \"cel\". Note: The loss function determines how the model's performance is measured during testing.</p> required <code>`--batch_size`</code> <code>int</code> <p>Specify the batch size for testing. Defaults to 32. Note: Batch size refers to the number of samples processed before the model's weights are updated.</p> required <code>`--data_folder`</code> <code>str</code> <p>The absolute path of the directory where the physioex dataset are stored, if None the home directory is used. Defaults to None. Note: Provide the path to the directory containing the datasets.</p> required <code>`--aggregate`</code> <code>bool</code> <p>Aggregate the results of the test. Defaults to False. Note: If specified, the test results will be aggregated across multiple datasets.</p> required <code>`--hpc`</code> <code>bool</code> <p>Using high performance computing setups or not, need to be called when datasets have been compressed into .h5 format with the compress_datasets command. Defaults to False. Note: Use this option if you are running the script on a high-performance computing cluster.</p> required <code>`--num_nodes`</code> <code>int</code> <p>Specify the number of nodes to be used for distributed testing, only used when hpc is True. Defaults to 1. Note: In slurm this value needs to be coherent with '--ntasks-per-node' or 'ppn' in torque. This option is relevant for distributed testing setups.</p> required <code>`--config`</code> <code>str</code> <p>Specify the path to the configuration file where to store the options to test the model with. Defaults to None. Note: The configuration file can override command line arguments.</p> required Example <pre><code>$ test_model --model tinysleepnet --loss cel --sequence_length 21 --selected_channels EEG --checkpoint_path /path/to/checkpoint\n</code></pre> <p>This command tests the <code>tinysleepnet</code> model using the CrossEntropy Loss</p> Notes <ul> <li>Ensure that the datasets are properly formatted and stored in the specified data folder using the preprocess script.</li> <li>The script supports both single-node and multi-node testing setups.</li> <li>The configuration file, if provided, should be in YAML format and contain valid key-value pairs for the script options.</li> </ul> <p><code>finetune</code> CLI </p> <p>Finetuning script for training and testing a model.</p> <p>This script allows you to fine-tune a pre-trained model using specified configurations and parameters.</p> Usage <p><code>$ finetune [PARAMS]</code> You can use the <code>finetune -h --help</code> command to access the command documentation.</p> <p>Parameters:</p> Name Type Description Default <code>--model</code> <code>str</code> <p>Specify the model to train, can be a yaml file if the model is not registered. Defaults to \"chambon2018\". If a yaml file is provided, it should contain the model configuration details.</p> required <code>--learning_rate</code> <code>float</code> <p>Specify the learning rate for the model. Defaults to 1e-7. Note: A smaller learning rate is often used for fine-tuning to avoid large updates that could disrupt the pre-trained weights.</p> required <code>--checkpoint_path</code> <code>str</code> <p>Specify the model checkpoint, if None physioex searches into its pretrained models. Defaults to None. Note: Provide the path to a specific checkpoint file to resume training from a saved state.</p> required <code>--checkpoint_dir</code> <code>str</code> <p>Specify the checkpoint directory where to store the new finetuned model checkpoints. Defaults to None. Note: This directory will be used to save checkpoints during training.</p> required <code>--datasets</code> <code>list</code> <p>Specify the datasets list to train the model on. Defaults to ['mass']. Note: Provide a list of dataset names to be used for training.</p> required <code>--selected_channels</code> <code>list</code> <p>Specify the channels to train the model. Defaults to ['EEG']. Note: Channels refer to the data modalities (e.g., EEG, EOG) used for training.</p> required <code>--sequence_length</code> <code>int</code> <p>Specify the sequence length for the model. Defaults to 21. Note: Sequence length refers to the number of time steps in each input sequence.</p> required <code>--loss</code> <code>str</code> <p>Specify the loss function to use. Defaults to \"cel\". Note: The loss function determines how the model's performance is measured during training.</p> required <code>--max_epoch</code> <code>int</code> <p>Specify the maximum number of epochs for training. Defaults to 20. Note: An epoch is one complete pass through the training dataset.</p> required <code>--num_validations</code> <code>int</code> <p>Specify the number of validations steps to be done in each epoch. Defaults to 10. Note: Validation steps are used to evaluate the model's performance on a validation set during training.</p> required <code>--batch_size</code> <code>int</code> <p>Specify the batch size for training. Defaults to 32. Note: Batch size refers to the number of samples processed before the model's weights are updated.</p> required <code>--data_folder</code> <code>str</code> <p>The absolute path of the directory where the physioex dataset are stored, if None the home directory is used. Defaults to None. Note: Provide the path to the directory containing the datasets.</p> required <code>--test</code> <code>bool</code> <p>Test the model after training. Defaults to False. Note: If specified, the model will be tested on the validation set after training.</p> required <code>--aggregate</code> <code>bool</code> <p>Aggregate the results of the test. Defaults to False. Note: If specified, the test results will be aggregated across multiple datasets.</p> required <code>--hpc</code> <code>bool</code> <p>Using high performance computing setups or not, need to be called when datasets have been compressed into .h5 format with the compress_datasets command. Defaults to False. Note: Use this option if you are running the script on a high-performance computing cluster.</p> required <code>--num_nodes</code> <code>int</code> <p>Specify the number of nodes to be used for distributed training, only used when hpc is True. Defaults to 1. Note: In slurm this value needs to be coherent with '--ntasks-per-node' or 'ppn' in torque. This option is relevant for distributed training setups.</p> required <code>--config</code> <code>str</code> <p>Specify the path to the configuration file where to store the options to train the model with. Defaults to None. Note: The configuration file can override command line arguments.</p> required Example <p><pre><code>$ finetune --model tinysleepnet --loss cel --sequence_length 21 --selected_channels EEG --checkpoint_path /path/to/checkpoint\n</code></pre> This command fine-tunes the <code>tinysleepnet</code> model using the CrossEntropy Loss (<code>cel</code>), with a sequence length of 21 and the <code>EEG</code> channel, starting from the specified checkpoint.</p> Notes <ul> <li>Ensure that the datasets are properly formatted and stored in the specified data folder using the preprocess script.</li> <li>The script supports both single-node and multi-node training setups.</li> <li>The configuration file, if provided, should be in YAML format and contain valid key-value pairs for the script options.</li> </ul>"},{"location":"pages/train/networks/","title":"Networks Module Overview","text":"<p>The model implemented into the PhysioEx library are:</p> <ul> <li>Chambon2018 model for sleep stage classification ( raw time series as input).</li> <li>TinySleepNet model for sleep stage classification (raw time series as input).</li> <li>SeqSleepNet model for sleep stage classification (time-frequency images as input).</li> </ul> Model Model Name Input Transform Target Transform Chambon2018 chambon2018 raw get_mid_label TinySleepNet tinysleepnet raw None SeqSleepNet seqsleepnet xsleepnet None <p>The models in PhysioEx are designed to receive input sequences of 30-second sleep epochs. These sequences can be either preprocessed or raw, depending on the specific requirements of the model.  The preprocessing status of the input sequences is indicated by the \"Input Transform\" attribute. This attribute must match the \"preprocessing\" argument of the dataset to ensure that the model receives as input the correct information.</p> <p>Similarly, models can be sequence-to-sequence (default) or sequence-to-epoch. In the last case a function that selects one epoch in the sequence needs to be added to the PhysioExDataModule pipeline to match the target data and the output of the model. These functions are implemented into the <code>physioex.train.networks.utils.target_transform</code> module. </p> <p>When implementing your own SleepModule, the Input Transform and Target Transform methods must be configurated properly, the best practice is to set them into a <code>.yaml</code> file as discussed in the train module documentation page.</p>"},{"location":"pages/train/networks/#extending-the-sleepmodule","title":"Extending the SleepModule","text":"<p>All the models compatible with PhysioEx are Pytorch Lightning Modules which extends the <code>physioes.train.networks.base.SleepModule</code>.</p> <p>By extending the module you can implement your own custom sleep staging deep learning network. When extending the module use a dictionary <code>module_config: dict</code> as the argument to the construct to allow compatibility with all the library. Second define your custom <code>torch.nn.Module</code> and use  <code>module_config: dict</code> as its constructor argument too.</p> <p>Example</p> <pre><code>import torch\nfrom physioex.train.networks.base import SleepModule\n\nclass CustomNet( torch.nn.Module ):\n    def __init__(self, module_config: dict):\n\n        # tipycally here you have an epoch_encoder and a sequence_encoder\n        self.epoch_encoder = ...\n        self.sequence_encoder = ...\n\n        pass\n\n    def forward(self, x : torch.Tensor):\n        encoding, preds = self.encode(x)\n        return preds\n\n    def encode(self, x : torch.Tensor):\n        # get your latent-space encodings\n        encodings = ...\n\n        # get your predictions out of the encodings\n        preds = ...\n\n        return econdings, preds\n\nclass CustomModule(SleepModule):\n    def __init__(self, module_config: dict):\n        super(CustomNet, self).__init__(CustomNet(module_config), module_config)\n</code></pre> <p>The SleepModule needs to know the <code>n_classes</code> ( for sleep staging this is tipycally 5 ) and the loss to be computed during training. By default the loss function in PhysioEx ( check <code>physioex.train.networks.utils.loss</code> ) take a python <code>dict</code> in its constructor, so you should always specify in your module_config the <code>n_classes</code> value, <code>loss_call</code> and <code>loss_params</code>.</p> <p><code>SleepModule</code> </p> <p>               Bases: <code>LightningModule</code></p> <p>A PyTorch Lightning module for sleep stage classification and regression tasks.</p> <p>This module is designed to handle both classification and regression experiments for sleep stage analysis. It leverages PyTorch Lightning for training, validation, and testing, and integrates various metrics for performance evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>`nn`</code> <code>Module</code> <p>The neural network model to be used for sleep stage analysis.</p> required <code>`config`</code> <code>Dict</code> <p>A dictionary containing configuration parameters for the module. Must include: - <code>n_classes</code> (int): The number of classes for classification tasks. If <code>n_classes</code> is 1, the module performs regression. - <code>loss_call</code> (callable): A callable that returns the loss function. - <code>loss_params</code> (dict): A dictionary of parameters to be passed to the loss function.</p> required <p>Attributes:</p> Name Type Description <code>`nn`</code> <code>Module</code> <p>The neural network model.</p> <code>`n_classes`</code> <code>int</code> <p>The number of classes for classification tasks.</p> <code>`loss`</code> <code>callable</code> <p>The loss function.</p> <code>`module_config`</code> <code>Dict</code> <p>The configuration dictionary.</p> <code>`learning_rate`</code> <code>float</code> <p>The learning rate for the optimizer. Default is 1e-4.</p> <code>`weight_decay`</code> <code>float</code> <p>The weight decay for the optimizer. Default is 1e-6.</p> <code>`val_loss`</code> <code>float</code> <p>The best validation loss observed during training.</p> Example <pre><code>import torch.nn as nn\nfrom your_module import SleepModule\n\nconfig = {\n    \"n_classes\": 5,\n    \"loss_call\": nn.CrossEntropyLoss,\n    \"loss_params\": {}\n}\n\nmodel = SleepModule(nn=YourNeuralNetwork(), config=config)\n</code></pre> Notes <ul> <li>This module supports both classification and regression tasks. The behavior is determined by the <code>n_classes</code> argument of the config dictionary.</li> <li>Various metrics are logged during training, validation, and testing to monitor performance.</li> <li>The learning rate scheduler is configured to reduce the learning rate when the validation loss plateaus.</li> </ul>"},{"location":"pages/train/networks/#network.SleepModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configures the optimizer and learning rate scheduler.</p> <p>Returns:</p> Type Description <p>Tuple[List[Optimizer], List[Dict]]: A tuple containing the optimizer and the learning rate scheduler.</p>"},{"location":"pages/train/networks/#network.SleepModule.forward","title":"<code>forward(x)</code>","text":"<p>Defines the forward pass of the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The output tensor.</p>"},{"location":"pages/train/networks/#network.SleepModule.encode","title":"<code>encode(x)</code>","text":"<p>Encodes the input data using the neural network.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The encoded tensor.</p>"},{"location":"pages/train/networks/#network.SleepModule.compute_loss","title":"<code>compute_loss()</code>","text":"<p>Computes the loss and logs metrics.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Tensor</code> <p>The embeddings tensor.</p> required <code>outputs</code> <code>Tensor</code> <p>The outputs tensor.</p> required <code>targets</code> <code>Tensor</code> <p>The targets tensor.</p> required <code>log</code> <code>str</code> <p>The log prefix. Defaults to \"train\".</p> required <code>log_metrics</code> <code>bool</code> <p>Whether to log additional metrics. Defaults to False.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The computed loss.</p>"},{"location":"pages/train/networks/#network.SleepModule.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Defines a single training step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>The input batch.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The computed loss.</p>"},{"location":"pages/train/networks/#network.SleepModule.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Defines a single validation step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Tuple[Tensor, Tensor]</code> <p>The input batch.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The computed loss.</p>"},{"location":"pages/train/networks/#network.SleepModule.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Defines a single test step.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>The input batch.</p> required <code>batch_idx</code> <code>int</code> <p>The index of the batch.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The computed loss.</p>"},{"location":"pages/train/train/","title":"Train Module Overview","text":"<p>The <code>physioex.train</code> module provides comprehensive tools for training, fine-tuning, and evaluating deep learning models on sleep staging datasets. </p>"},{"location":"pages/train/train/#usage-example","title":"Usage Example","text":"<p>Below is an example of how to use the training module:</p> <pre><code>from physioex.train.utils import train, test, finetune\nfrom physioex.data import PhysioExDataModule\n\nfrom physioex.train.networks.utils.loss import config as loss_config\nfrom physioex.train.networks import config as network_config\n\ncheckpoint_path = \"/path/to/your/checkpoint/dir/\"\n\n# first configure the model\n\n# set the model configuration dictonary\n\n# in case your model is from physioex.train.networks\n# you can load its configuration\n\nyour_model_config = network_config[\"tinysleepnet\"] \n\nyour_model_config[\"loss_call\"] = loss_config[\"cel\"] # CrossEntropy Loss\nyour_model_config[\"loss_params\"] = dict()\nyour_model_config[\"seq_len\"] = 21 # needs to be the same as the DataModule\nyour_model_config[\"in_channels\"] = 1 # needs to be the same as the DataModule\n\n#your_model_config[\"n_classes\"] = 5  needs to be set if you are loading a custom SleepModule\n</code></pre> <p>Here we loaded che configuration setup to train a <code>physioex.train.networks.TinySleepNet</code> model. Best practices is to have a .yaml file where to store the model configuration, both in case you are using a custom  <code>SleepModule</code> or in case you are using one of the models provided by PhysioEx. </p> <p>Here is an example of a possible .yaml configuration file and how to read it properly:</p>  .yaml .py <pre><code>module_config: \n    loss_call : physioex.train.networks.utils.loss:CrossEntropyLoss\n    loss_params : {}\n    seq_len : 21\n    in_channels : 1\n    n_classes : 5\n    ... # your additional model configuration parameters should be provided here\nmodule: physioex.train.networks:TinySleepNet # can be any model extends SleepModule\nmodel_name: \"tinysleepnet\" # can be avoided if loading your custom SleepModule\ninput_transform: \"raw\"\ntarget_transform: null\ncheckpoint_path : \"/path/to/your/checkpoint/dir/\"\n</code></pre> <pre><code>import yaml\n\nwith open(\"my_network_config.yaml\", \"r\") as file:\n    config = yaml.safe_load(file)\n\nyour_module_config = config[\"module_config\"]\n\n# load the loss function \nimport importlib\nloss_package, loss_class = your_module_config[\"loss_call\"].split(\":\")\nyour_model_config = getattr(importlib.import_module(loss_package), loss_class)\n\n# in case you provide model_name the system loads the additional model parameters from the library\nif \"model_name\" in config:\n    model_name = config[\"model_name\"]\n    module_config = networks_config[model_name][\"module_config\"]\n    your_model_config.update(module_config)\n\n    config[\"input_transform\"] = networks_config[model_name][\"input_transform\"]\n    config[\"target_transform\"] = networks_config[model_name][\"target_transform\"]\n\n# load the model class\nmodel_package, model_class = config[\"module\"].split(\":\")\nmodel_class = getattr(importlib.import_module(model_package), model_class)\n</code></pre> Note <p>In case you are using a model provided by the library, \"input_transform\" and \"target_transform\" can be loaded from the network_config ( line 20-21 )</p> <p>Now we need to set up the datamodule arguments and we can start training the model:</p> <p><pre><code>datamodule_kwargs = {\n    \"selected_channels\" : [\"EEG\"], # needs to match in_channels\n    \"sequence_length\" : your_model_config[\"seq_len\"],\n    \"target_transform\" : your_model_config[\"target_transform\"]\n    \"preprocessing\" : your_model_config[\"input_transform\"],\n    \"data_folder\" : \"/your/data/folder\",\n}\n\nmodel_config = your_model_config\n\n# Train the model\nbest_checkpoint = train(\n    datasets = \"hmc\", # can be a list or a PhysioExDataModule\n    datamodule_kwargs = datamodule_kwargs,\n    model = model,\n    model_class = model_class,\n    model_config = model_config,\n    checkpoint_path = checkpoint_path\n    batch_size = 128,\n    max_epochs = 10\n)\n\n# Test the model\nresults_dataframe = test(\n    datasets = \"hmc\",\n    datamodule_kwargs = datamodule_kwargs,\n    model_class = model_class,\n    model_config = your_model_config,\n    chekcpoint_path = os.path.join( checkpoint_path, best_checkpoint ),\n    batch_size = 128,\n    results_dir = checkpoint_path,  # if you want to save the test results \n                                    # in your checkpoint directory\n)\n</code></pre> Even in this case, the best practice should be to save the datamodule_kwargs into the .yaml configuration file, at least the non-dynamic ones. </p> <p>Now imagine that we want to fine-tune the trained model on a new dataset.</p> <pre><code>train_kwargs = {\n    \"dataset\" = \"dcsm\",\n    \"datamodule\" = datamodule_kwargs,\n    \"batch_size\" = 128,\n    \"max_epochs\" = 10,\n    \"checkpoint_path\" = checkpoint_path,\n}\n\nnew_best_checkpoint = finetune(\n    model_class = model_class,\n    model_config = model_config,\n    model_checkpoint = os.path.join( checkpoint_path, best_checkpoint),\n    learning_rate = 1e-7, # slower the learning rate to avoid losing prior training info.\n    train_kwargs = train_kwargs,\n) \n</code></pre>"},{"location":"pages/train/train/#documentation","title":"Documentation","text":"<p><code>train</code> </p> <p>Trains a model using the provided datasets and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[List[str], str, PhysioExDataModule]</code> <p>The datasets to be used for training. Can be a list of dataset names, a single dataset name, or a PhysioExDataModule instance.</p> required <code>datamodule_kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the PhysioExDataModule. Defaults to {}.</p> required <code>model</code> <code>SleepModule</code> <p>The model to be trained. If provided, <code>model_class</code>, <code>model_config</code>, and <code>resume</code> are ignored. Defaults to None.</p> required <code>model_class</code> <code>type</code> <p>The class of the model to be trained. Required if <code>model</code> is not provided. Defaults to None.</p> required <code>model_config</code> <code>dict</code> <p>The configuration dictionary for the model. Required if <code>model</code> is not provided. Defaults to None.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to be used for training. Defaults to 128.</p> required <code>fold</code> <code>int</code> <p>The fold index for cross-validation. Defaults to -1.</p> required <code>hpc</code> <code>bool</code> <p>Whether to use high-performance computing (HPC) settings. Defaults to False.</p> required <code>num_validations</code> <code>int</code> <p>The number of validation steps per epoch. Defaults to 10.</p> required <code>checkpoint_path</code> <code>str</code> <p>The path to save the model checkpoints. If None, a new path is generated. Defaults to None.</p> required <code>max_epochs</code> <code>int</code> <p>The maximum number of epochs for training. Defaults to 10.</p> required <code>num_nodes</code> <code>int</code> <p>The number of nodes to be used for distributed training. Defaults to 1.</p> required <code>resume</code> <code>bool</code> <p>Whether to resume training from the last checkpoint. Defaults to True.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path to the best model checkpoint.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>datasets</code> is not a list, a string, or a PhysioExDataModule instance.</p> <code>ValueError</code> <p>If <code>model</code> is None and any of <code>model_class</code> or <code>model_config</code> are also None.</p> Notes <ul> <li>The function sets up the data module, model, and trainer, and then starts the training process.</li> <li>If <code>resume</code> is True and a checkpoint is found, training resumes from the last checkpoint.</li> <li>The function returns the path to the best model checkpoint based on validation accuracy.</li> </ul> <p><code>test</code> </p> <p>Tests a model using the provided datasets and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[List[str], str, PhysioExDataModule]</code> <p>The datasets to be used for testing. Can be a list of dataset names, a single dataset name, or a PhysioExDataModule instance.</p> required <code>datamodule_kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the PhysioExDataModule. Defaults to {}.</p> required <code>model</code> <code>SleepModule</code> <p>The model to be tested. If provided, <code>model_class</code>, <code>model_config</code>, and <code>resume</code> are ignored. Defaults to None.</p> required <code>model_class</code> <code>type</code> <p>The class of the model to be tested. Required if <code>model</code> is not provided. Defaults to None.</p> required <code>model_config</code> <code>dict</code> <p>The configuration dictionary for the model. Required if <code>model</code> is not provided. Defaults to None.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to be used for testing. Defaults to 128.</p> required <code>fold</code> <code>int</code> <p>The fold index for cross-validation. Defaults to -1.</p> required <code>hpc</code> <code>bool</code> <p>Whether to use high-performance computing (HPC) settings. Defaults to False.</p> required <code>checkpoint_path</code> <code>str</code> <p>The path to the checkpoint from which to load the model. Required if <code>model</code> is not provided. Defaults to None.</p> required <code>results_path</code> <code>str</code> <p>The path to save the test results. If None, results are not saved. Defaults to None.</p> required <code>num_nodes</code> <code>int</code> <p>The number of nodes to be used for distributed testing. Defaults to 1.</p> required <code>aggregate_datasets</code> <code>bool</code> <p>Whether to aggregate the datasets for testing. Defaults to False.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing the test results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>datasets</code> is not a list, a string, or a PhysioExDataModule instance.</p> <code>ValueError</code> <p>If <code>model</code> is None and any of <code>model_class</code> or <code>model_config</code> are also None.</p> Notes <ul> <li>The function sets up the data module, model, and trainer, and then starts the testing process.</li> <li>The function returns a DataFrame containing the test results for each dataset.</li> <li>If <code>results_path</code> is provided, the results are saved as a CSV file in the specified path.</li> </ul> <p><code>finetune</code> </p> <p>Fine-tunes a pre-trained model using the provided datasets and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Union[List[str], str, PhysioExDataModule]</code> <p>The datasets to be used for fine-tuning. Can be a list of dataset names, a single dataset name, or a PhysioExDataModule instance.</p> required <code>datamodule_kwargs</code> <code>dict</code> <p>Additional keyword arguments to be passed to the PhysioExDataModule. Defaults to {}.</p> required <code>model</code> <code>Union[dict, SleepModule]</code> <p>The model to be fine-tuned. If provided, <code>model_class</code>, <code>model_config</code>, and <code>model_checkpoint</code> are ignored. Defaults to None.</p> required <code>model_class</code> <code>type</code> <p>The class of the model to be fine-tuned. Required if <code>model</code> is not provided. Defaults to None.</p> required <code>model_config</code> <code>dict</code> <p>The configuration dictionary for the model. Required if <code>model</code> is not provided. Defaults to None.</p> required <code>model_checkpoint</code> <code>str</code> <p>The path to the checkpoint from which to load the model. Required if <code>model</code> is not provided. Defaults to None.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate to be set for fine-tuning. If <code>None</code>, the learning rate is not updated. Default is 1e-7.</p> required <code>weight_decay</code> <code>Union[str, float]</code> <p>The weight decay to be set for fine-tuning. If <code>None</code>, the weight decay is not updated. If \"auto\", it is set to 10% of the learning rate. Default is \"auto\".</p> required <code>train_kwargs</code> <code>Dict</code> <p>Additional keyword arguments to be passed to the <code>train</code> function. Defaults to {}.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The path of the best model checkpoint.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>model</code> is <code>None</code> and any of <code>model_class</code>, <code>model_config</code>, or <code>model_checkpoint</code> are also <code>None</code>.</p> <code>ValueError</code> <p>If <code>model</code> is not a dictionary or a <code>SleepModule</code>.</p> Notes <ul> <li>Models cannot be fine-tuned from scratch; they must be loaded from a checkpoint or be a pre-trained model from <code>physioex.models</code>.</li> <li>Typically, when fine-tuning a model, you want to set up the learning rate.</li> </ul>"},{"location":"pages/train/networks/chambon2018/","title":"<code>Chambon2018</code> documentation","text":"<p>This page details the implementation of the <code>chambon2018</code> model published here.</p> <p>To train the model one could use the <code>train -experiment chambon2018</code> command.</p>"},{"location":"pages/train/networks/chambon2018/#physioex.train.networks.chambon2018.Chambon2018Net","title":"<code>physioex.train.networks.chambon2018.Chambon2018Net</code>","text":"<p>               Bases: <code>SleepModule</code></p> Source code in <code>physioex/train/networks/chambon2018.py</code> <pre><code>class Chambon2018Net(SleepModule):\n    def __init__(self, module_config: dict = module_config):\n        \"\"\"\n        The Chambon2018Net class extends SleepModule. This class is a wrapper for the core Chambon2018 network to be trained inside physioex.\n\n        Args:\n            module_config (dict): A dictionary containing the module configuration. Defaults to `module_config`.\n\n        Attributes:\n            all the attributes are from `SleepModule`\n        \"\"\"\n\n        module_config[\"n_times\"] = 3000\n        super(Chambon2018Net, self).__init__(Net(module_config), module_config)\n\n    def compute_loss(\n        self,\n        embeddings,\n        outputs,\n        targets,\n        log: str = \"train\",\n        log_metrics: bool = False,\n    ):\n        \"\"\"\n        Computes the loss for the Chambon2018Net model. This is necessary because Chambon2018 is a multi-input-single-output model, while the base class is a multi-input-multi-output model ( sequence-to-sequence ).\n\n        Args:\n            embeddings (torch.Tensor): The embeddings tensors.\n            outputs (torch.Tensor): The model output tensors.\n            targets (torch.Tensor): The target tensors.\n            log (str): The logging information. Defaults to \"train\".\n            log_metrics (bool): Whether to log metrics. Defaults to False.\n\n        Returns:\n            torch.Tensor: The computed loss value.\n        \"\"\"\n        batch_size, n_class = outputs.size()\n        outputs = outputs.reshape(batch_size, 1, n_class)\n        embeddings = embeddings.reshape(batch_size, 1, -1)\n\n        return super().compute_loss(embeddings, outputs, targets, log, log_metrics)\n</code></pre>"},{"location":"pages/train/networks/chambon2018/#physioex.train.networks.chambon2018.Chambon2018Net.__init__","title":"<code>__init__(module_config=module_config)</code>","text":"<p>The Chambon2018Net class extends SleepModule. This class is a wrapper for the core Chambon2018 network to be trained inside physioex.</p> <p>Parameters:</p> Name Type Description Default <code>module_config</code> <code>dict</code> <p>A dictionary containing the module configuration. Defaults to <code>module_config</code>.</p> <code>module_config</code> Source code in <code>physioex/train/networks/chambon2018.py</code> <pre><code>def __init__(self, module_config: dict = module_config):\n    \"\"\"\n    The Chambon2018Net class extends SleepModule. This class is a wrapper for the core Chambon2018 network to be trained inside physioex.\n\n    Args:\n        module_config (dict): A dictionary containing the module configuration. Defaults to `module_config`.\n\n    Attributes:\n        all the attributes are from `SleepModule`\n    \"\"\"\n\n    module_config[\"n_times\"] = 3000\n    super(Chambon2018Net, self).__init__(Net(module_config), module_config)\n</code></pre>"},{"location":"pages/train/networks/chambon2018/#physioex.train.networks.chambon2018.Chambon2018Net.compute_loss","title":"<code>compute_loss(embeddings, outputs, targets, log='train', log_metrics=False)</code>","text":"<p>Computes the loss for the Chambon2018Net model. This is necessary because Chambon2018 is a multi-input-single-output model, while the base class is a multi-input-multi-output model ( sequence-to-sequence ).</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Tensor</code> <p>The embeddings tensors.</p> required <code>outputs</code> <code>Tensor</code> <p>The model output tensors.</p> required <code>targets</code> <code>Tensor</code> <p>The target tensors.</p> required <code>log</code> <code>str</code> <p>The logging information. Defaults to \"train\".</p> <code>'train'</code> <code>log_metrics</code> <code>bool</code> <p>Whether to log metrics. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>torch.Tensor: The computed loss value.</p> Source code in <code>physioex/train/networks/chambon2018.py</code> <pre><code>def compute_loss(\n    self,\n    embeddings,\n    outputs,\n    targets,\n    log: str = \"train\",\n    log_metrics: bool = False,\n):\n    \"\"\"\n    Computes the loss for the Chambon2018Net model. This is necessary because Chambon2018 is a multi-input-single-output model, while the base class is a multi-input-multi-output model ( sequence-to-sequence ).\n\n    Args:\n        embeddings (torch.Tensor): The embeddings tensors.\n        outputs (torch.Tensor): The model output tensors.\n        targets (torch.Tensor): The target tensors.\n        log (str): The logging information. Defaults to \"train\".\n        log_metrics (bool): Whether to log metrics. Defaults to False.\n\n    Returns:\n        torch.Tensor: The computed loss value.\n    \"\"\"\n    batch_size, n_class = outputs.size()\n    outputs = outputs.reshape(batch_size, 1, n_class)\n    embeddings = embeddings.reshape(batch_size, 1, -1)\n\n    return super().compute_loss(embeddings, outputs, targets, log, log_metrics)\n</code></pre>"},{"location":"pages/train/networks/chambon2018/#physioex.train.networks.chambon2018.Net","title":"<code>physioex.train.networks.chambon2018.Net</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>physioex/train/networks/chambon2018.py</code> <pre><code>class Net(nn.Module):\n    def __init__(self, module_config=module_config):\n        \"\"\"\n        The Net class extends nn.Module. This class implements the core network proposed by Chambon et al in 2018.\n        The network consist of an encoder of epochs, a concatenation layer and a classification layer.\n        Multiple epochs are concatenated and fed to a linear classifier which predicts the sleep stage of the middle epoch of the sequence.\n\n        Args:\n            module_config (dict): A dictionary containing the module configuration. Defaults to `module_config`.\n\n        Attributes:\n            epoch_encoder (SleepStagerChambon2018): The epoch encoder.\n            clf (nn.Linear): The linear classifier.\n            drop (nn.Dropout): A dropout module to prevent overfitting.\n        \"\"\"\n        super().__init__()\n\n        print(module_config[\"in_channels\"])\n        self.epoch_encoder = SleepStagerChambon2018(\n            n_chans=module_config[\"in_channels\"],\n            sfreq=module_config[\"sf\"],\n            n_outputs=module_config[\"n_classes\"],\n            n_times=module_config[\"n_times\"],\n            return_feats=True,\n        )\n\n        self.clf = nn.Linear(\n            self.epoch_encoder.len_last_layer * module_config[\"sequence_length\"],\n            module_config[\"n_classes\"],\n        )\n\n        self.drop = nn.Dropout(0.5)\n\n    def forward(self, x):\n        \"\"\"\n        Implements the forward pass of the module.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output tensor of the module.\n        \"\"\"\n        x, y = self.encode(x)\n        return y\n\n    def encode(self, x: torch.Tensor):\n        \"\"\"\n        Encodes the input x using the epoch encoder, returns both the econdings and the classification outcome.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            tuple: A tuple containing the encoded input tensor and the output tensor of the module.\n        \"\"\"\n        batch_size, seqlen, nchan, nsamp = x.size()\n\n        x = x.reshape(-1, nchan, nsamp)\n\n        x = self.epoch_encoder(x)\n\n        x = x.reshape(batch_size, -1)\n\n        y = self.drop(x)\n        y = self.clf(y)\n\n        return x, y\n</code></pre>"},{"location":"pages/train/networks/chambon2018/#physioex.train.networks.chambon2018.Net.__init__","title":"<code>__init__(module_config=module_config)</code>","text":"<p>The Net class extends nn.Module. This class implements the core network proposed by Chambon et al in 2018. The network consist of an encoder of epochs, a concatenation layer and a classification layer. Multiple epochs are concatenated and fed to a linear classifier which predicts the sleep stage of the middle epoch of the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>module_config</code> <code>dict</code> <p>A dictionary containing the module configuration. Defaults to <code>module_config</code>.</p> <code>module_config</code> <p>Attributes:</p> Name Type Description <code>epoch_encoder</code> <code>SleepStagerChambon2018</code> <p>The epoch encoder.</p> <code>clf</code> <code>Linear</code> <p>The linear classifier.</p> <code>drop</code> <code>Dropout</code> <p>A dropout module to prevent overfitting.</p> Source code in <code>physioex/train/networks/chambon2018.py</code> <pre><code>def __init__(self, module_config=module_config):\n    \"\"\"\n    The Net class extends nn.Module. This class implements the core network proposed by Chambon et al in 2018.\n    The network consist of an encoder of epochs, a concatenation layer and a classification layer.\n    Multiple epochs are concatenated and fed to a linear classifier which predicts the sleep stage of the middle epoch of the sequence.\n\n    Args:\n        module_config (dict): A dictionary containing the module configuration. Defaults to `module_config`.\n\n    Attributes:\n        epoch_encoder (SleepStagerChambon2018): The epoch encoder.\n        clf (nn.Linear): The linear classifier.\n        drop (nn.Dropout): A dropout module to prevent overfitting.\n    \"\"\"\n    super().__init__()\n\n    print(module_config[\"in_channels\"])\n    self.epoch_encoder = SleepStagerChambon2018(\n        n_chans=module_config[\"in_channels\"],\n        sfreq=module_config[\"sf\"],\n        n_outputs=module_config[\"n_classes\"],\n        n_times=module_config[\"n_times\"],\n        return_feats=True,\n    )\n\n    self.clf = nn.Linear(\n        self.epoch_encoder.len_last_layer * module_config[\"sequence_length\"],\n        module_config[\"n_classes\"],\n    )\n\n    self.drop = nn.Dropout(0.5)\n</code></pre>"},{"location":"pages/train/networks/chambon2018/#physioex.train.networks.chambon2018.Net.forward","title":"<code>forward(x)</code>","text":"<p>Implements the forward pass of the module.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The output tensor of the module.</p> Source code in <code>physioex/train/networks/chambon2018.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Implements the forward pass of the module.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output tensor of the module.\n    \"\"\"\n    x, y = self.encode(x)\n    return y\n</code></pre>"},{"location":"pages/train/networks/chambon2018/#physioex.train.networks.chambon2018.Net.encode","title":"<code>encode(x)</code>","text":"<p>Encodes the input x using the epoch encoder, returns both the econdings and the classification outcome.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing the encoded input tensor and the output tensor of the module.</p> Source code in <code>physioex/train/networks/chambon2018.py</code> <pre><code>def encode(self, x: torch.Tensor):\n    \"\"\"\n    Encodes the input x using the epoch encoder, returns both the econdings and the classification outcome.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        tuple: A tuple containing the encoded input tensor and the output tensor of the module.\n    \"\"\"\n    batch_size, seqlen, nchan, nsamp = x.size()\n\n    x = x.reshape(-1, nchan, nsamp)\n\n    x = self.epoch_encoder(x)\n\n    x = x.reshape(batch_size, -1)\n\n    y = self.drop(x)\n    y = self.clf(y)\n\n    return x, y\n</code></pre>"}]}